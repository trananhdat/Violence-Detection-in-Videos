{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Copy of Copy of Drive FUSE example.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "c99EvWo1s9-x",
        "colab_type": "code",
        "outputId": "6e006512-11fa-4b91-cfb0-57a0b2e729df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4SJ-tGNkOeY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import project zip file\n",
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/ViolenceDetection.zip\", 'r')\n",
        "\n",
        "# extract in tmp folder\n",
        "zip_ref.extractall(\"/tmp\")\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JeLUAiVffwI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# change current directory to tmp\n",
        "import os\n",
        "os.chdir(\"/tmp\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNuMI5EQf7oy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import python files\n",
        "import src\n",
        "import settings\n",
        "import Deploy\n",
        "import Train\n",
        "import Evaluate\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8BHFw2Tgcwy",
        "colab_type": "code",
        "outputId": "1e1de398-9156-4527-b046-0a56da06926c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# install required packages\n",
        "!pip install sk-video"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sk-video\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/3f/ce848b8b2062ad1ccf1449094a740c775f6c761339f411e44f1e090f23a7/sk_video-1.1.10-py2.py3-none-any.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 34.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sk-video) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sk-video) (1.16.4)\n",
            "Installing collected packages: sk-video\n",
            "Successfully installed sk-video-1.1.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vg9zuhSYIztQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8a5e6dff-5a05-46cb-cbc0-fabdc9b1a471"
      },
      "source": [
        "# reset all variables if needed\n",
        "%reset"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfm4iQiMOSJ7",
        "colab_type": "text"
      },
      "source": [
        "## Update Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YqM3kTdlkvk",
        "colab_type": "code",
        "outputId": "e32c9a2e-bd40-40a3-f538-c31fd9f1226a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile src/net/G2D19_P2OF_ResHB_1LSTM.py\n",
        "\n",
        "import tensorflow as tf\n",
        "from src.net.NetBase import *\n",
        "from src.layers.LayerHelper import *\n",
        "from src.layers.BasicLayers import *\n",
        "from src.layers.ResidualLayers import *\n",
        "from src.layers.RNN import *\n",
        "import settings.LayerSettings as layerSettings\n",
        "import settings.DataSettings as dataSettings\n",
        "import numpy as np\n",
        "\n",
        "DARKNET19_MODEL_PATH = 'src//data//pretrainModels//darknet19//darknet19.pb'\n",
        "\n",
        "class Net(NetworkBase):\n",
        "\tdef __init__(self, inputImage_, batchSize_, unrolledSize_, isTraining_, trainingStep_):\n",
        "\t\tself._inputImage = inputImage_\n",
        "\t\tself._batchSize = batchSize_\n",
        "\t\tself._unrolledSize = unrolledSize_\n",
        "\t\tself._isTraining = isTraining_\n",
        "\t\tself._trainingStep = trainingStep_\n",
        "\t\n",
        "\t\tself._DROPOUT_PROB = 0.5\n",
        "\t\tself._NUMBER_OF_NEURONS_IN_LSTM = 1024\n",
        "\n",
        "\t\tself._dictOfInterestedActivations = {}\n",
        "\n",
        "\t\tif dataSettings.GROUPED_SIZE != 2:\n",
        "\t\t\terrorMessage = __name__ + \" only take GROUPED_SIZE = 2;\\n\"\n",
        "\t\t\terrorMessage += \"However, DataSettings.GROUPED_SIZE = \" + str(dataSettings.GROUPED_SIZE)\n",
        "\t\t\traise ValueError(errorMessage)\n",
        "\n",
        "\tdef Build(self):\n",
        "\t\tdarknet19_GraphDef = tf.GraphDef()\n",
        "\n",
        "\t\t'''\n",
        "\t\t      The CNN only take input shape [..., w, h, c].  Thus, move the UNROLLED_SIZE dimension\n",
        "\t\t    to merged with BATCH_SIZE, and form the shape: [b*u, w, h, c].\n",
        "\t\t'''\n",
        "\t\tconvInput = tf.reshape(self._inputImage, [-1,\n",
        "\t\t\t\t\t\t\t  dataSettings.IMAGE_SIZE, dataSettings.IMAGE_SIZE, dataSettings.IMAGE_CHANNELS])\n",
        "\n",
        "\t\twith tf.name_scope(\"DarkNet19\"):\n",
        "\t\t\twith open(DARKNET19_MODEL_PATH, 'rb') as modelFile:\n",
        "\t\t\t\tdarknet19_GraphDef.ParseFromString(modelFile.read())\n",
        "\t\t\t\tlistOfOperations = tf.import_graph_def(darknet19_GraphDef,\n",
        "\t\t\t\t\t\t\t\t\tinput_map={\"input\": convInput},\n",
        "#\t\t\t\t\t\t\t\t\treturn_elements=[\"2-maxpool\", \"32-leaky\"])\n",
        "\t\t\t\t\t\t\t\t\treturn_elements=[\"2-maxpool\", \"36-leaky\"])\n",
        "\t\t\t\tpool2 = listOfOperations[0].outputs[0]\n",
        "\t\t\t\tlastOp = listOfOperations[-1]\n",
        "\t\t\t\tout = lastOp.outputs[0]\n",
        "\t\t\t\tout, updateOp0 = BatchNormalization('BN_0', out, isConvLayer_=True,\n",
        "\t\t\t\t\t\t\t\t     isTraining_=self._isTraining, currentStep_=self._trainingStep)\n",
        "\t\t\t\n",
        "\n",
        "\t\topticalFlowOut, updateOF = self._buildOpticalFlowNet(pool2)\n",
        "\t\tprint(\"opticalFlowOut.shape = \", opticalFlowOut.shape)  # shape = [b*u, 1024]\n",
        "\n",
        "\t\twith tf.name_scope(\"CNN\"):\n",
        "\t\t\t'''\n",
        "\t\t\t    The input shape = [b, u, g, w, h, c]\n",
        "\t\t\t    after Conv, shape = [b*u*g, w', h', c']\n",
        "\t\t\t    here, decouple the Group dimension, shape = [b*u, g * w' * h' * c']\n",
        "\t\t\t'''\n",
        "\t\t\tprint(\"In CNN:\")\n",
        "\t\t\tprint(\"\\t darknetOutput.shape = \", out.shape)   # shape = [b*u*g, 7, 7, 1024]\n",
        "\t\t\tw, h, c = out.shape[1:]  # 7, 7, 1024\n",
        "\t\t\tout = tf.reshape( out,\n",
        "\t\t\t\t\t  [self._batchSize * self._unrolledSize, dataSettings.GROUPED_SIZE,\n",
        "\t\t\t\t\t  w, h, c])  # [b*u, g, 7, 7, 1024]\n",
        "\t\t\tout = tf.transpose(out, perm=[0, 2, 3, 4, 1])  # [b*u, 7, 7, 1024, g]\n",
        "\t\t\tprint(\"\\t after transpose, out.shape = \", out.shape)\n",
        "\t\t\tout = tf.reshape( out, \n",
        "\t\t\t\t\t  [self._batchSize * self._unrolledSize,\n",
        "\t\t\t\t\t   w, h, c * dataSettings.GROUPED_SIZE])\n",
        "\t\t\tprint(\"\\t before ConcatConv, out.shape = \", out.shape)  # shape = [b*u, 7, 7, 1024*g]\n",
        "\n",
        "\t\t\tout, updateOp1 = ResidualHeadBlock('ResHeadBlock', out, [128, 64, 128], isTraining_=self._isTraining,\n",
        "\t\t\t\t\t\t\ttrainingStep_=self._trainingStep, activationType_=\"LEAKY_RELU\", isTrainable_=True)\n",
        "\n",
        "\t\t\tprint(\"\\t before Fc, out.shape = \", out.shape)\n",
        "\t\t\tout = tf.cond(self._isTraining, lambda: tf.nn.dropout(out, self._DROPOUT_PROB), lambda: out)\n",
        "\n",
        "\t\t\tout = FullyConnectedLayer('Fc1', out, numberOfOutputs_=1024)\n",
        "\t\t\tout, updateOp2 = BatchNormalization('BN_4', out, isConvLayer_=False,\n",
        "\t\t\t\t\t\t\t     isTraining_=self._isTraining, currentStep_=self._trainingStep)\n",
        "\t\t\tself._dictOfInterestedActivations['CNN'] = out\n",
        "\t\t\tprint(\"\\t Fc out.shape = \", out.shape)\n",
        "\n",
        "\n",
        "\t\twith tf.name_scope(\"Concat\"):\n",
        "\t\t\tout = tf.concat([out, opticalFlowOut], axis=1)\n",
        "\t\t\tprint(\"In Concat:\")\n",
        "\t\t\tprint(\"\\t before Fc, out.shape = \", out.shape)\n",
        "\t\t\tout = FullyConnectedLayer('Fc', out, numberOfOutputs_=1024)\n",
        "\t\t\tout, updateOp3 = BatchNormalization('BN_5', out, isConvLayer_=False,\n",
        "\t\t\t\t\t\t\t     isTraining_=self._isTraining, currentStep_=self._trainingStep)\n",
        "\t\t\t'''\n",
        "\t\t\t    Note: For tf.nn.rnn_cell.dynamic_rnn(), the input shape of [1:] must be explicit.\n",
        "\t\t\t          i.e., one Can't Reshape the out by:\n",
        "\t\t\t\t  out = tf.reshape(out, [BATCH_SIZE, UNROLLED_SIZE, -1])\n",
        "\t\t\t\t  since '-1' is implicit dimension.\n",
        "\t\t\t'''\n",
        "\t\t\tfeaturesShapeInOneBatch = out.shape[1:].as_list()\n",
        "\t\t\ttargetShape = [self._batchSize, self._unrolledSize] + featuresShapeInOneBatch\n",
        "\t\t\tout = tf.reshape(out, targetShape)\n",
        "\t\t\tself._dictOfInterestedActivations['Concat'] = out\n",
        "\t\t\tprint(\"before LSTM, shape = \", out.shape)\n",
        "\n",
        "\n",
        "\t\tout, self._stateTensorOfLSTM_1, self._statePlaceHolderOfLSTM_1 = LSTM(\t\"LSTM_1\",\n",
        "\t\t\t\t\t\t\t\t\t\t\tout,\n",
        "\t\t\t\t\t\t\t\t\t\t\tself._NUMBER_OF_NEURONS_IN_LSTM,\n",
        "\t\t\t\t\t\t\t\t\t\t\tisTraining_=self._isTraining,\n",
        "\t\t\t\t\t\t\t\t\t\t\tdropoutProb_=self._DROPOUT_PROB)\n",
        "\n",
        "\t\tself._dictOfInterestedActivations['LSTM'] = out\n",
        "\n",
        "\t\twith tf.name_scope(\"Fc_Final\"):\n",
        "\t\t\tfeaturesShapeInOneBatch = out.shape[2:].as_list()\n",
        "\t\t\ttargetShape = [self._batchSize * self._unrolledSize] + featuresShapeInOneBatch\n",
        "\t\t\tout = tf.reshape(out, targetShape)\n",
        "\t\t\tout = FullyConnectedLayer('Fc3', out, numberOfOutputs_=dataSettings.NUMBER_OF_CATEGORIES)\n",
        "\t\t\tself._logits = tf.reshape(out, [self._batchSize, self._unrolledSize, -1])\n",
        "\n",
        "\t\tself._updateOp = tf.group(updateOF, updateOp0, updateOp1, updateOp2, updateOp3)\n",
        "\t\tprint()\n",
        "\n",
        "\n",
        "\n",
        "\t@property\n",
        "\tdef logitsOp(self):\n",
        "\t\treturn self._logits\n",
        "\n",
        "\t@property\n",
        "\tdef updateOp(self):\n",
        "\t\treturn self._updateOp\n",
        "\n",
        "\n",
        "\tdef GetListOfStatesTensorInLSTMs(self):\n",
        "\t\t'''\n",
        "\t\t    You should Not Only sess.run() the net.logits, but also this listOfTensors\n",
        "\t\t    to get the States of LSTM.  And assign it to PlaceHolder next time.\n",
        "\t\t    ex:\n",
        "\t\t\t>> tupleOfResults = sess.run( [out] + net.GetListOfStatesTensorInLSTMs(), ...)\n",
        "\t\t\t>> listOfResults = list(tupleOfResults)\n",
        "\t\t\t>> output = listOfResults.pop(0)\n",
        "\t\t\t>> listOfStates = listOfResults\n",
        "\n",
        "\t\t    See GetFeedDictOfLSTM() method as well\n",
        "\t\t'''\n",
        "\t\treturn [self._stateTensorOfLSTM_1]\n",
        "\n",
        "\n",
        "\tdef GetFeedDictOfLSTM(self, BATCH_SIZE_, listOfPreviousStateValues_=None):\n",
        "\t\t'''\n",
        "\t\t      This function will return a dictionary that contained the PlaceHolder-Value map\n",
        "\t\t    of the LSTM states.\n",
        "\t\t      You can use this function as follows:\n",
        "\t\t    >> feed_dict = { netInput : batchOfImages }\n",
        "\t\t    >> feedDictOFLSTM = net.GetLSTM_Feed_Dict(BATCH_SIZE, listOfPreviousStateValues)\n",
        "\t\t    >> tupleOfOutputs = sess.run( [out] + net.GetListOfStatesTensorInLSTMs(),\n",
        "\t\t\t\t\t\t  feed_dict = feed_dict.update(feedDictOFLSTM) ) \n",
        "\t\t    >> listOfOutputs = list(tupleOfOutputs)\n",
        "\t\t    >> output = listOfOutputs.pop(0)\n",
        "\t\t    >> listOfPreviousStateValues = listOfOutputs.pop(0)\n",
        "\t\t'''\n",
        "\t\tif listOfPreviousStateValues_ == None:\n",
        "\t\t\t'''\n",
        "\t\t\t    For the first time (or, the first of Unrolls), there's no previous state,\n",
        "\t\t\t    return zeros state.\n",
        "\t\t\t'''\n",
        "\t\t\tinitialCellState = tuple( [np.zeros([BATCH_SIZE_, self._NUMBER_OF_NEURONS_IN_LSTM])] * 2 )\n",
        "\t\t\tinitialCellState = tf.nn.rnn_cell.LSTMStateTuple(initialCellState[0], initialCellState[1])\n",
        "\n",
        "\t\t\treturn {self._statePlaceHolderOfLSTM_1 : initialCellState }\n",
        "\t\telse:\n",
        "\t\t\tif len(listOfPreviousStateValues_) != 1:\n",
        "\t\t\t\terrorMessage = \"len(listOfPreviousStateValues_) = \" + str( len(listOfPreviousStateValues_) )\n",
        "\t\t\t\terrorMessage += \"; However, the expected lenght is 1.\\n\"\n",
        "\t\t\t\terrorMessage += \"\\t Do you change the Network Structure, such as Add New LSTM?\\n\"\n",
        "\t\t\t\terrorMessage += \"\\t Or, do you add more tensor to session.run()?\\n\"\n",
        "\n",
        "\t\t\treturn { self._statePlaceHolderOfLSTM_1 : listOfPreviousStateValues_[0] }\n",
        "\n",
        "\n",
        "\tdef _buildOpticalFlowNet(self, inputTensor_):\n",
        "\t\t'''\n",
        "\t\t    The input shape = [b, u, g, w, h, c]\n",
        "\t\t    after Conv, shape = [b*u*g, w', h', c']\n",
        "\t\t    here, decouple the Group dimension, shape = [b*u, g * w' * h' * c']\n",
        "\t\t'''\n",
        "\t\twith tf.name_scope(\"OpticalFLow\"):\n",
        "\t\t\tprint(\"In OpticalFlow:\")\n",
        "\t\t\tprint(\"\\t pool2.shape = \", inputTensor_.shape)  # shape = [b*u*g, 112, 112, 32]\n",
        "\t\t\tw, h, c = inputTensor_.shape[1:]  # 112, 112, 32\n",
        "\t\t\tout = tf.reshape( inputTensor_,\n",
        "\t\t\t\t\t  [self._batchSize * self._unrolledSize, dataSettings.GROUPED_SIZE,\n",
        "\t\t\t\t\t  w, h, c])  # [b*u, g, 112, 112, 32]\n",
        "\t\t\tout = tf.transpose(out, perm=[0, 2, 3, 4, 1])  # [b*u, 112, 112, 32, g]\n",
        "\t\t\tprint(\"\\t after transpose, out.shape = \", out.shape)\n",
        "\t\t\tout = tf.reshape( out, \n",
        "\t\t\t\t\t  [self._batchSize * self._unrolledSize,\n",
        "\t\t\t\t\t   w, h, c * dataSettings.GROUPED_SIZE])\n",
        "\t\t\tprint(\"\\t before Conv2, out.shape = \", out.shape)  # shape = [b*u, 112, 112, 32*g]\n",
        "\t\t\tout = ConvLayer('Conv2', out, filterSize_=3, numberOfFilters_=64, stride_=1, padding_='SAME', isTrainable_=True)\n",
        "\t\t\tout, updateOp1 = BatchNormalization('BN2', out, isConvLayer_=True, isTraining_=self._isTraining,\n",
        "\t\t\t\t\t\t\t\t     currentStep_=self._trainingStep, isTrainable_=True)\n",
        "\t\t\tout = LeakyRELU('RELU2', out)\n",
        "\t\t\tout = MaxPoolLayer('Pool2', out, kernelSize_=2, stride_=2, padding_='SAME')\n",
        "\n",
        "\t\t\tout, updateOp2 = ResidualBlock('ResBlock', out, [128, 128, 64], isTraining_=self._isTraining,\n",
        "\t\t\t\t\t\t\ttrainingStep_=self._trainingStep, activationType_=\"LEAKY_RELU\", isTrainable_=True)\n",
        "\n",
        "\t\t\tout = MaxPoolLayer('Pool5', out, kernelSize_=2, stride_=2, padding_='SAME')\n",
        "\n",
        "\t\t\tprint(\"\\t before Fc, out.shape = \", out.shape)  # shape = [b*u, 28, 28, 128]\n",
        "\t\t\tout = tf.cond(self._isTraining, lambda: tf.nn.dropout(out, self._DROPOUT_PROB), lambda: out)\n",
        "\n",
        "\t\t\tout = FullyConnectedLayer('Fc_of', out, numberOfOutputs_=1024)\n",
        "\t\t\tout, updateOp3 = BatchNormalization('BN_of', out, isConvLayer_=False,\n",
        "\t\t\t\t\t\t\t     isTraining_=self._isTraining, currentStep_=self._trainingStep)\n",
        "\t\t\tself._dictOfInterestedActivations['OpticalFlow'] = out\n",
        "\t\t\tprint(\"\\t Fc final.shape = \", out.shape)\n",
        "\t\t\tupdateOp = tf.group(updateOp1, updateOp2, updateOp3)\n",
        "\t\t\treturn out, updateOp\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting src/net/G2D19_P2OF_ResHB_1LSTM.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ccwl1d0POXil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile Train.py\n",
        "\n",
        "#!/usr/bin/python3\n",
        "\n",
        "from src.Evaluator import *\n",
        "from src.Classifier import Classifier\n",
        "from src.Trainer import Trainer\n",
        "import settings.DataSettings as dataSettings\n",
        "import time\n",
        "\n",
        "class Main:\n",
        "\tdef __init__(self):\n",
        "\t\tclassifier = Classifier()\n",
        "\t\tclassifier.Build()\n",
        "\n",
        "\t\t# Trainer, Evaluator\n",
        "\t\tprint(\"Reading Training set...\")\n",
        "\t\tself.trainer = Trainer(classifier)\n",
        "\t\tself.trainEvaluator = Evaluator(\"train\", dataSettings.PATH_TO_TRAIN_SET_CATELOG, classifier)\n",
        "\t\tprint(\"\\t Done.\\n\")\n",
        "\n",
        "\t\tprint(\"Reading Validation set...\")\n",
        "\t\tself.validationEvaluator = Evaluator(\"validation\", dataSettings.PATH_TO_VAL_SET_CATELOG, classifier)\n",
        "\t\tprint(\"\\t Done.\\n\")\n",
        "\n",
        "\t\tprint(\"Reading Test set...\")\n",
        "\t\tself.testEvaluator = Evaluator(\"test\", dataSettings.PATH_TO_TEST_SET_CATELOG, classifier)\n",
        "\t\tprint(\"\\t Done.\\n\")\n",
        "\n",
        "\t\t# Summary\n",
        "\t\tsummaryOp = tf.summary.merge_all()\n",
        "\t\tself.trainer.SetMergedSummaryOp(summaryOp)\n",
        "\t\tself.trainEvaluator.SetMergedSummaryOp(summaryOp)\n",
        "\t\tself.validationEvaluator.SetMergedSummaryOp(summaryOp)\n",
        "\t\tself.bestThreshold = None\n",
        "\t\tself.testEvaluator.SetMergedSummaryOp(summaryOp)\n",
        "\n",
        "\t\t# Time\n",
        "\t\tself._startTrainEpochTime = time.time()\n",
        "\t\tself._trainCountInOneEpoch = 0\n",
        "\n",
        "\t\t# Saver\n",
        "\t\tself.modelSaver = tf.train.Saver(max_to_keep=trainSettings.MAX_TRAINING_SAVE_MODEL)\n",
        "\n",
        "\t\t# Session\n",
        "\t\tself.session = tf.Session()\n",
        "\t\tinit = tf.global_variables_initializer()\n",
        "\t\tself.session.run(init)\n",
        "\n",
        "\t\tself.trainer.SetGraph(self.session.graph)\n",
        "\t\tself.validationEvaluator.SetGraph(self.session.graph)\n",
        "\n",
        "\tdef __del__(self):\n",
        "\t\tself.session.close()\n",
        "\n",
        "\tdef Run(self):\n",
        "\t\tself.recoverFromPretrainModelIfRequired()\n",
        "\n",
        "\t\tself.calculateValidationBeforeTraining()\n",
        "\t\tself.resetTimeMeasureVariables()\n",
        "\n",
        "\t\tprint(\"Path to save mode: \", trainSettings.PATH_TO_SAVE_MODEL)\n",
        "\t\tprint(\"\\nStart Training...\\n\")\n",
        "\n",
        "\t\twhile self.trainer.currentEpoch < trainSettings.MAX_TRAINING_EPOCH:\n",
        "\t\t\tself.trainer.PrepareNewBatchData()\n",
        "\t\t\tself.trainer.Train(self.session)\n",
        "\t\t\tself._trainCountInOneEpoch += 1\n",
        "\n",
        "\t\t\tif self.trainer.isNewEpoch:\n",
        "\t\t\t\tprint(\"Epoch:\", self.trainer.currentEpoch, \"======================================\"\n",
        "\t\t\t\t\t+ \"======================================\"\n",
        "\t\t\t\t\t+ \"======================================\")\n",
        "\n",
        "\t\t\t\tself.printTimeMeasurement()\n",
        "\t\t\t\tself.trainer.PauseDataLoading()\n",
        "\n",
        "\t\t\t\tself.evaluateValidationSetAndPrint(self.trainer.currentEpoch)\n",
        "\t\t\t\tself.evaluateTrainingSetAndPrint(self.trainer.currentEpoch)\n",
        "\n",
        "\t\t\t\tif trainSettings.PERFORM_DATA_AUGMENTATION:\n",
        "\t\t\t\t\t# Preload TrainBatch while evaluate the TestSet\n",
        "\t\t\t\t\tself.trainer.ContinueDataLoading()\n",
        "\n",
        "\t\t\t\tself.evaluateTestSetAndPrint(self.trainer.currentEpoch)\n",
        "\n",
        "\t\t\t\tself.trainer.ContinueDataLoading()\n",
        "\n",
        "\t\t\t\tself.resetTimeMeasureVariables()\n",
        "\n",
        "\t\t\t\tif self.trainer.currentEpoch >= trainSettings.EPOCHS_TO_START_SAVE_MODEL:\n",
        "\t\t\t\t\tself.saveCheckpoint(self.trainer.currentEpoch)\n",
        "\t\tprint(\"Optimization finished.\")\n",
        "\t\tself.trainer.Release()\n",
        "\t\tself.trainEvaluator.Release()\n",
        "\t\tself.validationEvaluator.Release()\n",
        "\t\tself.testEvaluator.Release()\n",
        "\n",
        "\n",
        "\n",
        "\tdef recoverFromPretrainModelIfRequired(self):\n",
        "\t\tif trainSettings.PRETRAIN_MODEL_PATH_NAME != \"\":\n",
        "\t\t\tprint(\"Load Pretrain model from: \" + trainSettings.PRETRAIN_MODEL_PATH_NAME)\n",
        "\t\t\tlistOfAllVariables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
        "\t\t\tvariablesToBeRecovered = [ eachVariable for eachVariable in listOfAllVariables \\\n",
        "\t\t\t\t\t\t   if eachVariable.name.split('/')[0] not in \\\n",
        "\t\t\t\t\t\t   trainSettings.NAME_SCOPES_NOT_TO_RECOVER_FROM_CHECKPOINT ]\n",
        "\t\t\tmodelLoader = tf.train.Saver(variablesToBeRecovered)\n",
        "\t\t\tmodelLoader.restore(self.session, trainSettings.PRETRAIN_MODEL_PATH_NAME)\n",
        "\n",
        "\tdef evaluateTrainingSetAndPrint(self, currentEpoch_):\n",
        "\t\t'''\n",
        "\t\t    Since the BATCH_SIZE may be small (= 4 in my case), its BatchLoss or BatchAccuracy\n",
        "\t\t    may be fluctuated.  Calculate the whole Training Loss instead.\n",
        "\t\t    Note: If one want to calculate the BatchLoss ONLY, use Trainer.EvaluateTrainLoss().\n",
        "\t\t'''\n",
        "\t\tstartEvaluateTime = time.time()\n",
        "\t\tloss, frameAccuracy, threshold, videoAccuracy = self.trainEvaluator.Evaluate(\tself.session,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tcurrentEpoch_=currentEpoch_,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tthreshold_=self.bestThreshold)\n",
        "\t\tendEvaluateTime = time.time()\n",
        "\n",
        "\t\tself.printCalculationResults(jobType_='train', loss_=loss, frameAccuracy_=frameAccuracy,\n",
        "\t\t\t\t\t     isThresholdOptimized_=False,\n",
        "\t\t\t\t\t     threshold_=threshold, videoAccuracy_=videoAccuracy,\n",
        "\t\t\t\t\t     duration_=(endEvaluateTime-startEvaluateTime) )\n",
        "\n",
        "\n",
        "\tdef calculateValidationBeforeTraining(self):\n",
        "\t\tif trainSettings.PRETRAIN_MODEL_PATH_NAME != \"\":\n",
        "\t\t\tprint(\"Validation before Training \", \"=============================\"\n",
        "\t\t\t\t\t+ \"======================================\"\n",
        "\t\t\t\t\t+ \"======================================\")\n",
        "\t\t\tself.evaluateValidationSetAndPrint(currentEpoch_=0)\n",
        "\n",
        "\tdef evaluateValidationSetAndPrint(self, currentEpoch_):\n",
        "\t\tstartEvaluateTime = time.time()\n",
        "\t\tloss, frameAccuracy, threshold, videoAccuracy = self.validationEvaluator.Evaluate(self.session,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  currentEpoch_=currentEpoch_,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t  threshold_=None)\n",
        "\t\tendEvaluateTime = time.time()\n",
        "\n",
        "\t\tself.bestThreshold = threshold\n",
        "\t\tself.printCalculationResults(jobType_='validation', loss_=loss, frameAccuracy_=frameAccuracy,\n",
        "\t\t\t\t\t     isThresholdOptimized_=True,\n",
        "\t\t\t\t\t     threshold_=threshold, videoAccuracy_=videoAccuracy,\n",
        "\t\t\t\t\t     duration_=(endEvaluateTime-startEvaluateTime) )\n",
        "\n",
        "\tdef evaluateTestSetAndPrint(self, currentEpoch_):\n",
        "\t\tstartEvaluateTime = time.time()\n",
        "\t\tloss, frameAccuracy, threshold, videoAccuracy = self.testEvaluator.Evaluate(self.session,\n",
        "\t\t\t\t\t\t\t\t\t\t\t    currentEpoch_=currentEpoch_,\n",
        "\t\t\t\t\t\t\t\t\t\t\t    threshold_=self.bestThreshold)\n",
        "\t\tendEvaluateTime = time.time()\n",
        "\n",
        "\t\tself.printCalculationResults(jobType_='test', loss_=loss, frameAccuracy_=frameAccuracy,\n",
        "\t\t\t\t\t     isThresholdOptimized_=False,\n",
        "\t\t\t\t\t     threshold_=threshold, videoAccuracy_=videoAccuracy,\n",
        "\t\t\t\t\t     duration_=(endEvaluateTime-startEvaluateTime) )\n",
        "\n",
        "\tdef printTimeMeasurement(self):\n",
        "\t\ttimeForTrainOneEpoch = time.time() - self._startTrainEpochTime\n",
        "\t\tprint(\"\\t Back Propergation time measurement:\")\n",
        "\t\tprint(\"\\t\\t duration: \", \"{0:.2f}\".format(timeForTrainOneEpoch), \"s/epoch\")\n",
        "\t\taveragedTrainTime = timeForTrainOneEpoch / self._trainCountInOneEpoch\n",
        "\t\tprint(\"\\t\\t average: \", \"{0:.2f}\".format(averagedTrainTime), \"s/batch\")\n",
        "\t\tprint()\n",
        "\n",
        "\tdef resetTimeMeasureVariables(self):\n",
        "\t\tself._startTrainEpochTime = time.time()\n",
        "\t\tself._trainCountInOneEpoch = 0\n",
        "\n",
        "\tdef printCalculationResults(self, jobType_, loss_, frameAccuracy_, isThresholdOptimized_, threshold_, videoAccuracy_, duration_):\n",
        "\t\tfloatPrecision = \"{0:.4f}\"\n",
        "\t\tprint(\"\\t \"+jobType_+\":\")\n",
        "\t\tif isThresholdOptimized_:\n",
        "\t\t\tprint(\"\\t     loss:\", floatPrecision.format(loss_),\n",
        "\t\t\t\t\"     frame accuracy:\", floatPrecision.format(frameAccuracy_),\n",
        "\t\t\t\t\"     best frame threshold:\", threshold_,\n",
        "\t\t\t\t\"     video accuracy:\", floatPrecision.format(videoAccuracy_),\n",
        "\t\t\t\t\"     duration:\", \"{0:.2f}\".format(duration_) + \"(s)\\n\" )\n",
        "\t\telse:\n",
        "\t\t\tprint(\"\\t     loss:\", floatPrecision.format(loss_),\n",
        "\t\t\t\t\"     frame accuracy:\", floatPrecision.format(frameAccuracy_),\n",
        "\t\t\t\t\"     given frame threshold:\", threshold_,\n",
        "\t\t\t\t\"     video accuracy:\", floatPrecision.format(videoAccuracy_),\n",
        "\t\t\t\t\"     duration:\", \"{0:.2f}\".format(duration_) + \"(s)\\n\" )\n",
        "\n",
        "\n",
        "\tdef saveCheckpoint(self, currentEpoch_):\n",
        "\t\tpathToSaveCheckpoint = os.path.join(trainSettings.PATH_TO_SAVE_MODEL, \"save_epoch_\" + str(currentEpoch_) )\n",
        "\t\tcheckpointPathFileName = os.path.join(pathToSaveCheckpoint, \"ViolenceNet.ckpt\")\n",
        "\t\tself.modelSaver.save(self.session, checkpointPathFileName)\n",
        "\n",
        "\n",
        "def main():\n",
        "\tmain = Main()\n",
        "\tmain.Run()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Df6gV4AsOera",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile Evaluate.py\n",
        "\n",
        "#!/usr/bin/python3\n",
        "\n",
        "import sys\n",
        "import settings.EvaluationSettings as evalSettings\n",
        "from src.Classifier import Classifier\n",
        "from src.Evaluator import *\n",
        "import time\n",
        "\n",
        "\n",
        "def PrintResults(loss_, frameAccuracy_, isThresholdOptimized_, threshold_, videoAccuracy_, duration_):\n",
        "    floatPrecision = \"{0:.4f}\"\n",
        "    if isThresholdOptimized_:\n",
        "        print(\"\\t     loss:\", floatPrecision.format(loss_),\n",
        "              \"     frame accuracy:\", floatPrecision.format(frameAccuracy_),\n",
        "              \"     best frame threshold:\", threshold_,\n",
        "              \"     video accuracy:\", floatPrecision.format(videoAccuracy_),\n",
        "              \"     duration:\", \"{0:.2f}\".format(duration_) + \"(s)\\n\")\n",
        "    else:\n",
        "        print(\"\\t     loss:\", floatPrecision.format(loss_),\n",
        "              \"     frame accuracy:\", floatPrecision.format(frameAccuracy_),\n",
        "              \"     given frame threshold:\", threshold_,\n",
        "              \"     video accuracy:\", floatPrecision.format(videoAccuracy_),\n",
        "              \"     duration:\", \"{0:.2f}\".format(duration_) + \"(s)\\n\")\n",
        "\n",
        "\n",
        "def main(path_to_dataset_list = \"datalists//evaluate.txt\", find_threshold=False, threshold = 3):\n",
        "\n",
        "    PATH_TO_DATA_SET_CATELOG = path_to_dataset_list\n",
        "    classifier = Classifier()\n",
        "    classifier.Build()\n",
        "    evaluator = Evaluator(\"evaluate\", PATH_TO_DATA_SET_CATELOG, classifier)\n",
        "\n",
        "    with tf.Session() as session:\n",
        "        init = tf.global_variables_initializer()\n",
        "        session.run(init)\n",
        "\n",
        "        print(\"Load Model from: \", evalSettings.PATH_TO_MODEL_CHECKPOINTS)\n",
        "        modelLoader = tf.train.Saver()\n",
        "        modelLoader.restore(session, evalSettings.PATH_TO_MODEL_CHECKPOINTS)\n",
        "\n",
        "        startEvaluateTime = time.time()\n",
        "        if find_threshold:\n",
        "            print(\"Start evaluate: \", PATH_TO_DATA_SET_CATELOG, \", and find the best threshold...\")\n",
        "            loss, frameAccuracy, threshold, videoAccuracy = evaluator.Evaluate(session,\n",
        "                                                                               currentEpoch_=0,\n",
        "                                                                               threshold_=None)\n",
        "            endEvaluateTime = time.time()\n",
        "            PrintResults(loss_=loss, frameAccuracy_=frameAccuracy, isThresholdOptimized_=True,\n",
        "                         threshold_=threshold, videoAccuracy_=videoAccuracy,\n",
        "                         duration_=(endEvaluateTime - startEvaluateTime))\n",
        "\n",
        "        else:\n",
        "            threshold = threshold\n",
        "            print(\"Start evaluate: \", PATH_TO_DATA_SET_CATELOG, \", with threshold : \", threshold)\n",
        "            loss, frameAccuracy, threshold, videoAccuracy = evaluator.Evaluate(session,\n",
        "                                                                               currentEpoch_=0,\n",
        "                                                                               threshold_=threshold)\n",
        "            endEvaluateTime = time.time()\n",
        "            PrintResults(loss_=loss, frameAccuracy_=frameAccuracy, isThresholdOptimized_=False,\n",
        "                         threshold_=threshold, videoAccuracy_=videoAccuracy,\n",
        "                         duration_=(endEvaluateTime - startEvaluateTime))\n",
        "\n",
        "    evaluator.Release()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYZcd5TJOnUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile settings/DataSettings.py\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "PATH_TO_TRAIN_SET_CATELOG = 'datalists//train.txt'\n",
        "PATH_TO_VAL_SET_CATELOG = 'datalists//val.txt'\n",
        "PATH_TO_TEST_SET_CATELOG = 'datalists//test.txt'\n",
        "\n",
        "'''\n",
        "    The input will be (BATCH_SIZE, UNROLLED_SIZE, GROUPED_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "    'BATCH_SIZE': How many Videos should be used for a single step.\n",
        "    'UNROLLED_SIZE':  How many Frames should be extracted from a video.\n",
        "    'GROUPED_SIZE'  Some net need Many Frames (refered as the GROUPED_SIZE) to feed into the\n",
        "\t\t  net for one inference.  For example, the P2D19_1Fc_1LSTM take two frame\n",
        "\t\t  images as its input.\n",
        "'''\n",
        "GROUPED_SIZE = 2\n",
        "IMAGE_SIZE = 224\n",
        "IMAGE_CHANNELS = 3\n",
        "\n",
        "'''\n",
        "    In this project, categories = {NoFight, Fight}\n",
        "'''\n",
        "NUMBER_OF_CATEGORIES = 2\n",
        "\n",
        "\n",
        "#########################\n",
        "#  Uncommen Adjustment  #\n",
        "#########################\n",
        "NO_FIGHT_LABEL = [1., 0.]\n",
        "FIGHT_LABEL = [0., 1.]\n",
        "\n",
        "\n",
        "#####################\n",
        "# Advenced Settings #\n",
        "#####################\n",
        "FLOAT_TYPE = np.float32\n",
        "'''\n",
        "    Following control the timeout of LoadData Thread.\n",
        "    Recommand values:\n",
        "\tBATCH_SIZE=4, No DataAug, TIMEOUT_FOR_WAIT_QUEUE = 10\n",
        "\tBATCH_SIZE=4, DataAug, TIMEOUT_FOR_WAIT_QUEUE = 40\n",
        "\tBATCH_SIZE=20, No DataAug, TIMEOUT_FOR_WAIT_QUEUE = 20\n",
        "\tBATCH_SIZE=40, No DataAug, TIMEOUT_FOR_WAIT_QUEUE = 40\n",
        "\tBATCH_SIZE=40, DataAug, TIMEOUT_FOR_WAIT_QUEUE = 100\n",
        "'''\n",
        "TIMEOUT_FOR_WAIT_QUEUE = 100\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpexSOXAO7Zp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile settings/EvaluationSettings.py\n",
        "\n",
        "############################################################\n",
        "#  Settings for both Evaluation in Train.py & Evaluate.py  #\n",
        "############################################################\n",
        "'''\n",
        "    For evaluation, batchSize = 1 (i.e. pass the same video in the same batch).\n",
        "'''\n",
        "UNROLLED_SIZE = 50\n",
        "\n",
        "#####################\n",
        "# Advenced Settings #\n",
        "#####################\n",
        "WAITING_QUEUE_MAX_SIZE = 30\n",
        "LOADED_QUEUE_MAX_SIZE = 15\n",
        "NUMBER_OF_LOAD_DATA_THREADS=1\n",
        "\n",
        "\n",
        "##############################\n",
        "#  Settings for Evaluate.py  #\n",
        "##############################\n",
        "\n",
        "PATH_TO_MODEL_CHECKPOINTS = \"datalists//save_epoch_12//ViolenceNet.ckpt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rW9Wq-rAPRss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile settings/TrainSettings.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import settings.DataSettings as dataSettings\n",
        "\n",
        "'''\n",
        "      Following two variables control the shape of input\n",
        "    data as the shape: [BATCH_SIZE*UNROLLED_SIZE, w, h, c].\n",
        "    BATCH_SIZE: number of Videos in a batch.\n",
        "    UNROLLED_SIZE: number of Frames in a Video.\n",
        "      For the ConvNet part, the input will be the shape:\n",
        "    [BATCH_SIZE*UNROLLED_SIZE, w, h, c].\n",
        "      For the RNN part, the input will be the shape:\n",
        "    [BATCH_SIZE, UNROLLED_SIZE, w, h, c] so that the\n",
        "    tf.nn.rnn_cell.dynamic_rnn() can unroll the RNN.\n",
        "      The output of the total network will be the shape:\n",
        "    [BATCH_SIZE, UNROLLED_SIZE, NUMBER_OF_CATEGORIES]\n",
        "'''\n",
        "BATCH_SIZE = 4\n",
        "UNROLLED_SIZE = 40\n",
        "#BATCH_SIZE = 40\n",
        "#UNROLLED_SIZE = 2\n",
        "\n",
        "PRETRAIN_MODEL_PATH_NAME = \"datalists//save_epoch_12//ViolenceNet.ckpt\"\n",
        "#PRETRAIN_MODEL_PATH_NAME = \"temp/G2D19_P2OF_ResHB_1LSTM_dataAug_expLR/save_epoch_14/ViolenceNet.ckpt\"\n",
        "\n",
        "'''\n",
        "    If one want to finetune, insert the LastLayer to the following list.\n",
        "    ex: NAME_SCOPES_NOT_TO_RECOVER_FROM_CHECKPOINT = ['Conv4', 'Conv5']\n",
        "'''\n",
        "NAME_SCOPES_NOT_TO_RECOVER_FROM_CHECKPOINT = []\n",
        "\n",
        "MAX_TRAINING_EPOCH = 2\n",
        "\n",
        "EPOCHS_TO_START_SAVE_MODEL = 1\n",
        "PATH_TO_SAVE_MODEL = \"results//\"\n",
        "MAX_TRAINING_SAVE_MODEL = MAX_TRAINING_EPOCH\n",
        "PERFORM_DATA_AUGMENTATION = True\n",
        "\n",
        "def GetOptimizer(learningRate_):\n",
        "\treturn tf.train.AdamOptimizer(learning_rate=learningRate_)\n",
        "\n",
        "'''\n",
        "    Following list three different LearningRate decay methods:\n",
        "\t1. _stairLearningRate(),\n",
        "\t2. _exponentialDecayLearningRate()\n",
        "\t3. _polynomialDecayLearningRate()\n",
        "'''\n",
        "def _stairLearningRate(currentEpoch_, currentStep_):\n",
        "\t#LIST_OF_EPOCH_LEARNING_RATE_PAIRS = [ (0, 1e-4), (5, 1e-5) ]\n",
        "\tLIST_OF_EPOCH_LEARNING_RATE_PAIRS = [ (0, 1e-6), (15, 5e-7), (25, 1e-7) ]\n",
        "\t#LIST_OF_EPOCH_LEARNING_RATE_PAIRS = [ (0, 3e-6), (5, 2.5e-6), (10, 2e-6), (15, 1.5e-6), (20, 1e-6) ]\n",
        "\n",
        "\tfor eachPair in reversed(LIST_OF_EPOCH_LEARNING_RATE_PAIRS):\n",
        "\t\tif currentEpoch_ >= eachPair[0]:\n",
        "\t\t\treturn eachPair[1]\n",
        "\n",
        "\t# If nothing matched, return the first pair.learningRate as default\n",
        "\treturn LIST_OF_EPOCH_LEARNING_RATE_PAIRS[0][1] \n",
        "\n",
        "\n",
        "def _exponentialDecayLearningRate(currentEpoch_, currentStep_):\n",
        "\t'''\n",
        "\t    Exponential Decay:\n",
        "\t\tlearningRate = INITIAL_LEARNING_RATE * DECAY_RATE ^ (currentStep_ / DECAY_STEP) + END_LEARNING_RATE\n",
        "\t'''\n",
        "\tINITIAL_LEARNING_RATE = 1e-5\n",
        "\tDECAY_RATE = 0.9\n",
        "\tNUMBER_OF_BATCHES_PER_EPOCH = 250\n",
        "\tNUMBER_OF_EPOCHS_PER_DECAY = 1\n",
        "\tDECAY_STEP = int(NUMBER_OF_BATCHES_PER_EPOCH * NUMBER_OF_EPOCHS_PER_DECAY)\n",
        "\tEND_LEARNING_RATE = 0.0\n",
        "\n",
        "\tlearningRate = INITIAL_LEARNING_RATE * DECAY_RATE ** (currentStep_ / DECAY_STEP) + END_LEARNING_RATE\n",
        "\n",
        "\treturn learningRate\n",
        "\n",
        "def _polynomialDecayLearningRate(currentEpoch_, currentStep_):\n",
        "\t'''\n",
        "\t    Polynomial Decay:\n",
        "\t\tstep = min(currentStep_, MAX_STEPS)\n",
        "\t\tlearningRate = (START_LEARNING_RATE - END_LEARNING_RATE) * (1 - step/MAX_STEPS)^(POWER) + END_LEARNING_RATE\n",
        "\t'''\n",
        "\tSTART_LEARNING_RATE = 2e-6\n",
        "\tEND_LEARNING_RATE = 1e-7\n",
        "\tMAX_STEPS = MAX_TRAINING_EPOCH * 125\n",
        "\tPOWER = 4\n",
        "\n",
        "\n",
        "\n",
        "def GetLearningRate(currentEpoch_, currentStep_):\n",
        "#\treturn _stairLearningRate(currentEpoch_, currentStep_)\n",
        "\treturn _exponentialDecayLearningRate(currentEpoch_, currentStep_=currentStep_)\n",
        "\n",
        "\n",
        "\n",
        "#####################\n",
        "# Advenced Settings #\n",
        "#####################\n",
        "'''\n",
        "    Following settings depend on (BATCH_SIZE, UNROLLED_SIZE, PERFORM_DATA_AUGMENTATION):\n",
        "    if (4, 40, False), Recommend values:\n",
        "\tWAITING_QUEUE_MAX_SIZE = 60\n",
        "\tLOADED_QUEUE_MAX_SIZE = 30\n",
        "\tNUMBER_OF_LOAD_DATA_THREADS=2\n",
        "\n",
        "    if (4, 40, True), Recommend values:\n",
        "\tWAITING_QUEUE_MAX_SIZE = 180\n",
        "\tLOADED_QUEUE_MAX_SIZE = 80\n",
        "\tNUMBER_OF_LOAD_DATA_THREADS=2\n",
        "\n",
        "    if (40, 1, False), Recommend values:\n",
        "\tWAITING_QUEUE_MAX_SIZE = 180\n",
        "\tLOADED_QUEUE_MAX_SIZE = 80\n",
        "\tNUMBER_OF_LOAD_DATA_THREADS=4\n",
        "\n",
        "    if (40, 1, True), Recommend values:\n",
        "\tWAITING_QUEUE_MAX_SIZE = 180\n",
        "\tLOADED_QUEUE_MAX_SIZE = 80\n",
        "\tNUMBER_OF_LOAD_DATA_THREADS=4\n",
        "\n",
        "     Note: The \"Averaged GetBatch Time\" that printed while you train an epoch, should be\n",
        "\t   smaller than 0.001(s). Otherwise, increase NUMBER_OF_LOAD_DATA_THREADS.\n",
        "'''\n",
        "WAITING_QUEUE_MAX_SIZE = 180\n",
        "LOADED_QUEUE_MAX_SIZE = 80\n",
        "NUMBER_OF_LOAD_DATA_THREADS=4\n",
        "\n",
        "MAX_GRADIENT_VALUE = 5.0\n",
        "MIN_GRADIENT_VALUE = -5.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwKA--0cPe0H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use to delete a file\n",
        "# !rm src/net/G2D19_P2OF_ResHB_1LSTM.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C9YY9TrjWZX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use to view a file\n",
        "%pycat Evaluate.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3e2oiavf-te",
        "colab_type": "code",
        "outputId": "177d7301-093d-42c7-f4b3-436348c314a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        }
      },
      "source": [
        "Evaluate.main()"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Using Network:  src.net.G2D19_P2OF_ResHB_1LSTM \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-8c7015c705f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mEvaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/Evaluate.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(path_to_dataset_list, find_threshold, threshold)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mPATH_TO_DATA_SET_CATELOG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_dataset_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"evaluate\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH_TO_DATA_SET_CATELOG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/src/Classifier.py\u001b[0m in \u001b[0;36mBuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m                           \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munrolledSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUMBER_OF_CATEGORIES\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \t\t'''\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogitsOp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tf.nn.softmax\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/src/net/G2D19_P2OF_ResHB_1LSTM.py\u001b[0m in \u001b[0;36mBuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m \t\t\t\t\t\t\t  dataSettings.IMAGE_SIZE, dataSettings.IMAGE_SIZE, dataSettings.IMAGE_CHANNELS])\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DarkNet19\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDARKNET19_MODEL_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodelFile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                                 \u001b[0mdarknet19_GraphDef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParseFromString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\Internship\\\\Codes\\\\ViolenceDetection\\\\src\\\\data\\\\pretrainModels\\\\darknet19\\\\darknet19.pb'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQlKQhHN6oC2",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3KCTGGjikoy",
        "colab_type": "code",
        "outputId": "79c1a98a-34c6-4e57-bc6b-a92e13d438d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# list files in current working directory\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drivefs_ipc.0  drivefs_ipc.0_shell\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRLAIcSRB4IS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "d650b2e0-2f3b-4e82-ae66-32f8b402494b"
      },
      "source": [
        "# zip project folder\n",
        "!7z a tmp.7z tmp"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\n",
            "p7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,2 CPUs Intel(R) Xeon(R) CPU @ 2.30GHz (306F0),ASM,AES-NI)\n",
            "\n",
            "Scanning the drive:\n",
            "  0M Scan \b\b\b\b\b\b\b\b\b\b          \b\b\b\b\b\b\b\b\b\b65 folders, 2289 files, 5752314554 bytes (5486 MiB)\n",
            "\n",
            "Creating archive: tmp.7z\n",
            "\n",
            "Items to compress: 2354\n",
            "\n",
            "  0%\b\b\b\b    \b\b\b\b  0% 31 + tmp/.idea/workspace.xml\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  0% 46 + tmp/datalists/darknet19.pb\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  1% 46 + tmp/datalists/darknet19.pb\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  1% 47 + tmp/datalists/evaluate.txt\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                    \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  1% 48 + tmp/datalists/save_epoch_12/ViolenceNet.ckpt.data-00000-of-00001\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  2% 48 + tmp/datalists/save_epoch_12/ViolenceNet.ckpt.data-00000-of-00001\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  3% 48 + tmp/datalists/save_epoch_12/ViolenceNet.ckpt.data-00000-of-00001\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  4% 48 + tmp/datalists/save_epoch_12/ViolenceNet.ckpt.data-00000-of-00001\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  5% 48 + tmp/datalists/save_epoch_12/ViolenceNet.ckpt.data-00000-of-00001\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  6% 48 + tmp/datalists/save_epoch_12/ViolenceNet.ckpt.data-00000-of-00001\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  7% 48 + tmp/datalists/save_epoch_12/ViolenceNet.ckpt.data-00000-of-00001\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  8% 48 + tmp/datalists/save_epoch_12/ViolenceNet.ckpt.data-00000-of-00001\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b  9% 48 + tmp/datalists/save_epoch_12/ViolenceNet.ckpt.data-00000-of-00001\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 10% 48 + tmp/datalists/save_epoch_12/ViolenceNet.ckpt.data-00000-of-00001\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 11% 48 + tmp/datalists/save_epoch_12/ViolenceNet.ckpt.data-00000-of-00001\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 12% 48 + tmp/datalists/save_epoch_12/ViolenceNet.ckpt.data-00000-of-00001\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 13% 48 + tmp/datalists/save_epoch_12/ViolenceNet.ckpt.data-00000-of-00001\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b                                                                          \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 14% 48 + tmp/datalists/save_epoch_12/ViolenceNet.ckpt.data-00000-of-00001"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiYOi3LRv3l4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# download project zip file\n",
        "from google.colab import files\n",
        "files.download(\"tmp.7z\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fwsQFgZv8x5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# delete a folder\n",
        "# !rm -r tmp"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}